from dotenv import load_dotenv
from LLM.LLMClient import LLMClient
from LLM.utils import llm_utils
from services.database_client import database_client
from LLM.prompts.prompt_manager import PromptManager
import json
_ = load_dotenv()

class ragClient():
    def __init__(self, chatclient: LLMClient, llm_utils: llm_utils, database_client: database_client,  prompt_manager):
        self.chatclient = chatclient
        self.llm_utils = llm_utils
        self.database_client = database_client
        self.prompt_manager = prompt_manager
        
    def generate_rag_response(self, messages: list) -> str:
        """
        Generates a completion using the OpenAI API based on the user input.

        Args:
            user_input (str): The input provided by the user.

        Returns:
            str: The response generated by the OpenAI API.
        """
        response = self.chatclient.generate_chat_response(messages)
        tool_calls = [item for item in response.output if item.type == "function_call"]

        if tool_calls:
            tool_function_name = tool_calls[0].name
            arguments = json.loads(tool_calls[0].arguments)
            if tool_function_name == 'retrieve_information':
                tool_call_results = self.llm_utils.retrieve_information(search_query=arguments['search_query'])
                self.database_client.log_chatbot_interaction(arguments['search_query'], "tool_search_query", 0)
                messages = self.prompt_manager.update_messages_with_toolcalls(messages, tool_function_name, arguments['search_query'], tool_call_results)
                output = self.chatclient.generate_chat_response(messages)
        
        return output.output_text