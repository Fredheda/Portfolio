from dotenv import load_dotenv
from LLM.LLMClient import LLMClient
from LLM.utils import llm_utils
import json
_ = load_dotenv()

class ragClient():
    def __init__(self, chatclient: LLMClient, llm_utils: llm_utils,  tools):
        self.chatclient = chatclient
        self.llm_utils = llm_utils
        self.tools = tools
        
    def generate_rag_response(self, messages: list) -> str:
        """
        Generates a completion using the OpenAI API based on the user input.

        Args:
            user_input (str): The input provided by the user.

        Returns:
            str: The response generated by the OpenAI API.
        """
        response = self.chatclient.generate_chat_response(messages)
        tool_calls = response.tool_calls

        if tool_calls is not None:
            tool_function_name = tool_calls[0].function.name
            arguments = json.loads(tool_calls[0].function.arguments)
            if tool_function_name == 'retrieve_information':
                tool_call_results = self.llm_utils.retrieve_information(search_query=arguments['search_query'])
                messages.append({"role": "assistant", "content": "The retrieve information tool was called with the following arguments: search_query='quaternions'."})
                messages.append({"role": "assistant", "content": tool_call_results})
                response = self.chatclient.generate_chat_response(messages)
        
        return response.content